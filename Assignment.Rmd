---
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: header.tex
    latex_engine: xelatex
title: "STA 380 Take Home Exam , Book Verion -2"
author: "Viswa Tej Seela"
date: "31 July 2022"
geometry: margin=0.75in
fontsize: 10pt
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/ieee.csl
link-citations: true
references:
- id: Rpubs
  title: Rpubs Documentation
  URL: https://rpubs.com/
  publisher: Rpubs
  type: webpage
  
- id: NN
  title: NN 
  URL: https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/
  publisher: Rviews
  type: webpage   
---
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\begin{center}
\end{center}
<!-- Actual text starts here -->
\hypersetup{linkcolor = black }
\clearpage
\tableofcontents
\clearpage

```{r setup , echo=FALSE,include=FALSE,warning=FALSE,error=FALSE}
library(ISLR2)
library(RColorBrewer)
library(ggplot2)
library(glmnet)
library(pls)
library(rpart)
library(mlbench)
library(adabag)
library(e1071)
library(caret)
library(ipred)
library(dplyr)
library(tree)
library(keras)
library(tensorflow)
library(randomForest)
library(ggplot2)
library(ggcorrplot) 
library(MASS) 
library(reshape2) 
library(reshape) 

attach(Boston)
attach(Carseats)

require(MASS)
require(leaps)
require(glmnet)
require(gbm)
```

# Chapter 2
### Question 10
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
str(Boston)
```

### Q10-1  To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library.
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
dim(Boston) 
```
* 506 rows and 13 Columns , where row represent the set of predictor observations for a given Neighborhood in Boston and each column represents a Predictor Variable 

### Q10-2 Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center', fig.width= 12, fig.height= 12}
summary(Boston)
pairs(Boston,main="Scatterplot Matrix for all Predictors") 


M <-cor(Boston)
ggcorrplot(M,hc.order = TRUE,lab = TRUE)

par(mfrow = c(2, 2))

plot(Boston$crim, Boston$medv,
     xlab = 'Per Capita Crime Rate', 
     ylab = 'medv', 
     col = 'blue')



plot(Boston$rm, Boston$medv,
     xlab = 'Average number of rooms per dwelling', 
     ylab = 'medv', 
     col = 'blue')

plot(Boston$lstat, Boston$medv,
     xlab = 'Lower status of the population (percent)', 
     ylab = 'medv', 
     col = 'blue')

plot(Boston$ptratio, Boston$medv,
     xlab = 'Pupil-teacher ratio by town', 
     ylab = 'medv', 
     col = 'blue')
```

**Findings: **

* There is correlation between variables so first plotting a correlation matrix and then will deepdive into the relation between two variables

* Even though there are other varibles which have a high collinearity , i have chosen to demonstrate MEDV and DIS

* As crim increases, the medv decreases. demand for homes in more dangerous areas leads to a devaluation in the price of homes there.

* As rm increases, the medv also increases,homes with more square footage/space are valued higher than homes with less space.

* As lstat increases, the medv decreases, s the lower status of population (percent) increases, the median value of owner-occupied homes drops.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
par(mfrow = c(3, 2))
plot(Boston$age, Boston$dis,
     xlab = 'Proportion of owner-occupied units built prior to 1940', 
     ylab = 'dis', 
     col = 'red')

plot(Boston$indus, Boston$medv,
     xlab = 'proportion of non-retail business acres per town', 
     ylab = 'dis', 
     col = 'red')

plot(Boston$nox, Boston$medv,
     xlab = 'nitrogen oxides concentration (parts per 10 million)', 
     ylab = 'dis', 
     col = 'red')

plot(Boston$zn, Boston$medv,
     xlab = 'proportion of residential land zoned for lots over 25,000 sq.ft', 
     ylab = 'dis', 
     col = 'red')
plot(Boston$lstat, Boston$medv,
     xlab = 'lower status of the population (percent)', 
     ylab = 'dis', 
     col = 'red')


```
### Q10-3 Are any of the predictors associated with per capita crime rate? If so, explain the relationship.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
par(mfrow = c(2, 2))
plot(Boston$crim ~ Boston$zn,
     log = 'xy',
     col = 'steelblue')

plot(Boston$crim ~ Boston$age,
     log = 'xy',
     col = 'steelblue')

plot(Boston$crim ~ Boston$dis,
     log = 'xy',
     col = 'black')

plot(Boston$crim ~ Boston$lstat,
     log = 'xy',
     col = 'black')

```
**Findings**

* As the proportion of owner-occupied units built prior to 1940 increases, the Per Capita Crime Rate increases.

* As the weighted mean of distances to five Boston employment centres increases, the Per Capita Crime Rate decreases.

* As the lower status of the population (percent) increases, the Per Capita Crime Rate increases.

### Q10-4 Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
par(mfrow = c(2, 2))

ggplot(Boston, aes(x=crim)) +
  geom_histogram(position="identity", alpha=0.5,binwidth =5)

```
* There are very few Boston suburbs with high crime rates. Majority of observations align with a zero Per Capita Crime Rate.
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}

ggplot(Boston, aes(x=Boston$tax)) +
  geom_histogram(position="identity", alpha=0.5,binwidth =60)
```

* High count of observations that align with a near 700 value on the x-axis.A moderate count of observations that fall within the range of 200-400 values on the x-axis.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}

ggplot(Boston, aes(x=Boston$ptratio)) +
  geom_histogram(position="identity", alpha=0.5)
```
* There is a spike approximate to 20 on the x-axis that indicates a higher frequency of observations.

### Q10-5 How many of the census tracts in this data set bound the Charles river?
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}

nrow(subset(Boston, chas ==1))  #35
```

### Q10-6 What is the median pupil-teacher ratio among the towns in this data set?
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
summary(Boston$ptratio) #19.05
```
### Q10-7 Which census tract of Boston has lowest median value of owner- occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}

lowest_medv = Boston[order(Boston$medv), ]
lowest_medv[1, ] #399 suburb has lowest medv with medv 0f 5 ie. 5000$

summary(Boston) 

```

* 399 suburb has lowest medv with medv 0f 5 ie. 5000$

* More Crime, Less Rooms per Dwelling, Low Status of the Population (Percent), and Low Median Value of Owner Homes.

### Q10-8 In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
nrow(subset(Boston, rm  > 7)) 
nrow(subset(Boston, rm  > 8)) 

dwelling_8 = subset(Boston, rm > 8)
summary(dwelling_8) 
gc()
```


**Findings**

* 64 room have more than 7 rooms per dwelling

* 13 rooms have more than 8 dwelling

* Average Per Capita Crime Rate is quite low at 0.71879. (below Boston average)

* The typical percentage of owner-occupied housing constructed before 1940 is 71.54.

* has a typical owner-occupied home price of $44,000 on average (above the Boston average)

* has a 4.31 percent average lower status of the population (below the Boston average)

\newpage 

# Chapter 3

## Question 15

### a)  For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# Checking the correlation between the response variable and the predictors
# Plotting pairwise scatterplots to check the correlation
boston_treated = na.omit(Boston)
mtmelt <- melt(boston_treated, id = "crim")
ggplot(mtmelt, aes(x = value, y = crim)) +
  facet_wrap(~variable, scales = "free") +
  geom_point()
# Correlation
corr1 = cor(boston_treated)
corr1[1,]
predIdx <- c(2:14);
k = lapply(predIdx, function(x) lm(Boston$crim ~ Boston[, x],na.action = na.omit))
var_names <- names(Boston[,2:14])
names(k) = var_names
p_value = list()
r_squared = list()
coeff = list()
for (i in 1:13){
  p_value[i] = summary(k[[i]])$coefficients[2,4]
  r_squared[i]= summary(k[[i]])$r.squared[1]
  coeff[i] = summary(k[[i]])$coefficients[2,1]
}
results = cbind(var_names,p_value,r_squared)
single_pred_coeff = cbind(var_names,coeff)
library (plyr)
summary(k[[1]])
# Creating a column with all the regression coefficients
l = lapply(1:13, function(x) k[[x]]$coefficients[2])
names(l) = names(k)
sim_coeff <- ldply (l, data.frame)
sim_coeff
library(data.table)
setnames(sim_coeff, old=c(".id","X..i.."), new=c("Predictor", "sim_coeff"))
```
**Findings**:  

* Crime rate is positively influenced by rad and tax variables

* Fitting the linear regression model for each predictor   

* rad and tax predictors explain the maximum variability in the response variable with adjusted R^2 closer to 35%  

* All the predictors are statistically significant in explaining the response variable based on their p-value which is less than 0.05 except for chas

### b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?

```{r, ,echo=FALSE, fig.align='center' ,fig.width=11, fig.height=8}
set.seed(4)
lm.fit = lm(Boston$crim~.,Boston)
summary(lm.fit)
library(data.table)
# coefficients
l2 = lm.fit$coefficients
mul_coeff <- ldply (l2, data.frame)
setnames(mul_coeff, old=c(".id","X..i.."), new=c("Predictor", "mul_coeff"))
```
**Findings**:

* All the predictors combined can successfully explain close to 45% of the variation in the response variable  

* rad,dis,black,medv,zn have relatively high t-value suggesting that they have a good influence on the response variable  

* Null hypothesis can be rejected for rad,dis,black,medv,zn 

### c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regres- sion model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

```{r, ,echo=FALSE, fig.align='center' ,fig.width=4, fig.height=4}
# Inner join on sim_coeff and mul_coeff to get the data frame with both value in the same dataset
 i = merge(sim_coeff, mul_coeff, by = 'Predictor')
# Plotting a scatter plot
ggplot(i, aes(x=sim_coeff, y=mul_coeff)) +
  geom_point() + 
  geom_text(label=rownames(i))
```

* It looks like a few of the uni coefficient points change drastically. For example `nox` goes from +31 in the univariate case to -10 in the multivariate case.

### d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form Y =β0 +β1X+β2X2 +β3X3 +ε.

```{r, ,echo=FALSE}
predIdx <- c(2:14);
k = lapply(predIdx, function(x) lm(Boston$crim ~ Boston[, x]+I(Boston[, x]^2)+I(Boston[, x]^3)))
names(k) <- names(Boston[,2:14])
summary(k[[13]])
```

* nox,rm,age,dis and medv have a non linear relationship with crim variable as adjusted R2 has increased by fitting a non-linear model

\newpage

# Chapter 6 

## Question 9

```{r, echo=FALSE}
# Load ISLR Library
# Set the Seed for reproducibility
set.seed(11)
```

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# Type of data
str(College)

#Variables count in data
ncol(College)

# Scaling the data
College[, -1] <- apply(College[, -1], 2, scale)
```

### (a) Split the data set into a training set and a test set

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# Randomizing the split
train <-  sample(1:dim(College)[1], dim(College)[1] *0.75)
test <- -train

# Create the training and testing data
train_college <- College[train, ]
test_college <- College[test, ] # Remove the training data est data

# Observations in the training and testing data
nrow(train_college)  
nrow(test_college)  
```


### (b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# Linear model 
linear_model <- lm(Apps ~ ., data = train_college)
#summary(linear_model)
linear_prediction <- predict(linear_model, test_college)
linear_MSE <- mean((linear_prediction - test_college$Apps)^2)
linear_MSE
```

* The mean squared error or MSE is $`r linear_MSE`$, which is extremely large.


### (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.

* With lambda = grid we will implement a ridge regression over a a grid ov values ranging from 10^10 to 10^-2.  This way we cover the full range of scenarios from the null model containing only the intercept, to the least squares fit
# When alpha = 0 we fit a ridge regression

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# Create a model matrix from the train and test data
train_matrix <- model.matrix(Apps ~ ., data = train_college)
test_matrix <- model.matrix(Apps ~ ., data = test_college)
grid = 10^seq(10, -2, length=100)

ridge <- glmnet(train_matrix, train_college$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
plot(ridge)
# Run a k=fold cross validation for the ridge regression model 

cv_ridge  <- cv.glmnet(train_matrix, train_college[, "Apps"], alpha=0)
plot(cv_ridge)
best_lambda_ridge <- cv_ridge$lambda.min
best_lambda_ridge # The best lambda value!
```

* From above, we see that the value of λ that results in the smallest cross validation error for ridge regression is $`r best_lambda_ridge`$.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
ridge_prediction <- predict(ridge, newx = test_matrix, s = best_lambda_ridge)
ridge_MSE <- mean((ridge_prediction - test_college[, "Apps"])^2); ridge_MSE
```

* The test MSE is $`r ridge_MSE`$.

### (d) Fit a lasso model on the training set, with λ chosen by cross- validation. Report the test error obtained, along with the num- ber of non-zero coefficient estimates.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# When alpha = 1 we fit a lasso
lasso <- glmnet(train_matrix, train_college$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
plot(lasso)
# Run a k=fold cross validation for the ridge regression model 
# When alpha = 0 we fit a ridge regression
cv_lasso  <- cv.glmnet(train_matrix, train_college[, "Apps"], alpha=1)
plot(cv_ridge)
best_lambda_lasso <- cv_lasso$lambda.min
best_lambda_lasso # The best lambda value!
```

* From above, we see that the value of λ that results in the smallest cross validation error for lasso is $`r best_lambda_lasso`$.


```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# What is the test MSE associated with this best value of λ for lasso?
lasso_prediction <- predict(lasso, newx = test_matrix, s = best_lambda_lasso)
lasso_MSE <- mean((lasso_prediction - test_college[, "Apps"])^2); lasso_MSE
```

* The test MSE for lasso is $`r lasso_MSE`$.


### (e) Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# Fit a PCR Model
pcr_fit <- pcr(Apps ~ ., data = train_college, scale = TRUE, validation = "CV")
validationplot(pcr_fit, val.type = "MSEP")
```


```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# What is the test MSE associated with this best value of λ for PCR?
pcr_prediction <- predict(pcr_fit, test_college, ncomp = 7)
pcr_MSE <- mean((test_college[, "Apps"] - data.frame(pcr_prediction))^2); pcr_MSE
```

* The test MSE for PCR is $`r pcr_MSE`$ and the M values is 16 according to the graph



### (f) Fit a PLS model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
pls_fit <- plsr(Apps ~ ., data = train_college, scale = TRUE, validation = "CV")
validationplot(pls_fit, val.type = "MSEP")
```


```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
pls_prediction <- predict(pls_fit, test_college, ncomp = 7)
pls_MSE <- mean((test_college[, "Apps"] - data.frame(pls_prediction))^2); pls_MSE
```

* The test MSE for PLS is $`r pls_MSE`$.

### (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# To answer this, let's create a plot of the R-squared values and the MSE values
average_test <- mean(test_college[, "Apps"])
linear_r2 = 1 - mean((test_college[, "Apps"] - linear_prediction)^2)/mean((test_college[, "Apps"] - average_test)^2)
ridge_r2 = 1 - mean((test_college[, "Apps"] -ridge_prediction)^2) /mean((test_college[, "Apps"] - average_test)^2)
lasso_r2 = 1 - mean((test_college[, "Apps"] -lasso_prediction)^2) /mean((test_college[, "Apps"] - average_test)^2)
pcr_r2 = 1 - mean((test_college[, "Apps"] -data.frame(pcr_prediction))^2) /mean((test_college[, "Apps"] -average_test)^2)
pls_r2 = 1 - mean((test_college[, "Apps"] -data.frame(pls_prediction))^2) /mean((test_college[, "Apps"] -average_test)^2)

par(mfrow = c(1,2))
# Let's create a bar plot of the R2 values to visualize any differences 
barplot(c(linear_r2, ridge_r2, lasso_r2, pcr_r2, pls_r2), col="cornflowerblue", 
        names.arg=c("OLS","Ridge", "Lasso", "PCR", "PLS"), main = "Test R-Squared",
        ylab = "Test R-Squared", ylim = c(0,1))

barplot(c(linear_MSE, ridge_MSE, lasso_MSE, pcr_MSE, pls_MSE), col="orangered", 
        names.arg=c("OLS","Ridge", "Lasso", "PCR", "PLS"), main = "Test MSE",
        ylab = "Test MSE")
```

## Question 11

```{r, echo=FALSE}
data(Boston)
```

### (a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.


```{r, echo=FALSE}
# Split the data into train and test sets
set.seed(4)
train <- sample(1:nrow(Boston), nrow(Boston)*0.70)
test <- -train
y.test <- crim[test]
```

**Least squares**


```{r, echo=FALSE}
lm.fit <- lm(crim~., data=Boston, subset=train)
summary(lm.fit)
```
* The variables that are statistically significant are zn, dis, rad, lstat, and medv.

```{r, echo=FALSE}
lm.pred <- predict(lm.fit, Boston[test,])
lm.error <- mean((lm.pred - y.test)^2)
lm.error
```


**Best Subset Selection**

```{r, echo=FALSE}

best_subset <- regsubsets(crim ~ ., data=Boston, subset=train, nvmax=13)
best_subset_summary <- summary(best_subset)
```


```{r, echo=FALSE}
# Pick the best model using validation set approach
test_matrix <- model.matrix(crim~., data=Boston[test,])

val.errors <- rep(NA,13)
for(i in 1:13){
 coefi <- coef(best_subset,id=i)
 pred <- test_matrix[,names(coefi)]%*%coefi
 val.errors[i] <- mean((Boston$crim[test]-pred)^2) 
}

which.min(val.errors)
```
```{r, echo=FALSE}
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[6],col="red",cex=2,pch=20)
```


```{r, echo=FALSE}
coef(best_subset, which.min(val.errors))
```


```{r, echo=FALSE}
best_subset_mse <- val.errors[6]
best_subset_mse
```


```{r, echo=FALSE}
# Perform best subset selection with the chosen model on the entire dataset in order to obtain more accurate coefficient estimates

best_subset_full <- regsubsets(crim ~., data=Boston, nvmax = 13)
coef(best_subset_full, 6)
```

```{r, echo=FALSE}
# Choose the model using cross-validation and the best subset method

predict.regsubsets <- function (object, newdata, id,...){
 form<-as.formula(object$call [[2]])
 mat<-model.matrix(form,newdata)
 coefi<-coef(object, id=id)
 xvars<-names(coefi)
 mat[,xvars]%*%coefi 
}

k <- 10
set.seed(2)
folds <- sample(1:k,nrow(Boston), replace=TRUE)
cv.errors <- matrix(NA,k,13, dimnames=list(NULL, paste(1:13)))

for(j in 1:k){
  best.fit=regsubsets(crim~., data=Boston[folds!=j, ], nvmax=13)
  for(i in 1:13) {
    pred=predict(best.fit,Boston[folds==j,],id=i)
    cv.errors[j,i]=mean((Boston$crim[folds==j]-pred)^2)
  }
}
```

```{r, echo=FALSE}
mean.cv.errors <- apply(cv.errors,2,mean) 
mean.cv.errors
```

```{r, echo=FALSE}
plot(mean.cv.errors, type="b")
points(which.min(mean.cv.errors), mean.cv.errors[12], pch=20, col='red', cex=2)
```

```{r, echo=FALSE}
best_subset_cv <- regsubsets(crim~., data=Boston, nvmax=13)
coef(best_subset_cv, 12)
```
```{r, echo=FALSE}
best_subset_mse_cv <- mean.cv.errors[12]
best_subset_mse_cv
```

**Forward Stepwise**


```{r, echo=FALSE}
fwd <- regsubsets(crim~., data=Boston, subset=train, nvmax=13, method="forward")
```

```{r, echo=FALSE}
# Pick the best model using validation set approach
test_matrix <- model.matrix(crim~., data=Boston[test,])

val.errors <- rep(NA,13)
for(i in 1:13){
 coefi <- coef(fwd,id=i)
 pred <- test_matrix[,names(coefi)]%*%coefi
 val.errors[i] <- mean((Boston$crim[test]-pred)^2) 
}

which.min(val.errors)
```

```{r, echo=FALSE}
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[6],col="red",cex=2,pch=20)
```

```{r, echo=FALSE}
fwd_mse <- val.errors[6]
fwd_mse
```



```{r, echo=FALSE}
# Perform forward subset selection with the chosen model on the entire dataset in order to obtain more accurate coefficient estimates

fwd_full <- regsubsets(crim ~., data=Boston, nvmax = 13)
coef(fwd_full, 6)
```


* The models chosen by best subset selection, and forward selection using the validation approach chose the same variables - zn, nox, dis, rad, black, and medv. The test errors for both are the same.


```{r, echo=FALSE}
# Choose the model using cross-validation and the forward selection method

predict.regsubsets <- function (object, newdata, id,...){
 form<-as.formula(object$call [[2]])
 mat<-model.matrix(form,newdata)
 coefi<-coef(object, id=id)
 xvars<-names(coefi)
 mat[,xvars]%*%coefi 
}

k <- 10
set.seed(3)
folds <- sample(1:k,nrow(Boston), replace=TRUE)
cv.errors <- matrix(NA,k,13, dimnames=list(NULL, paste(1:13)))

for(j in 1:k){
  best.fit <- regsubsets(crim~., data=Boston[folds!=j, ], nvmax=13, method='forward')
  for(i in 1:13) {
    pred<- predict(best.fit,Boston[folds==j,],id=i)
    cv.errors[j,i] <- mean((Boston$crim[folds==j]-pred)^2)
  }
}
```

```{r, echo=FALSE}
mean.cv.errors.fwd <- apply(cv.errors,2,mean) 
mean.cv.errors.fwd
```

```{r, echo=FALSE}
which.min(mean.cv.errors.fwd)
```

```{r, echo=FALSE}
plot(mean.cv.errors.fwd, type="b")
points(which.min(mean.cv.errors.fwd), mean.cv.errors.fwd[2], pch=20, col='red', cex=2)
```

```{r, echo=FALSE}
fwd_cv <- regsubsets(crim~., data=Boston, nvmax=13, method='forward')
coef(fwd_cv, 2)
```


```{r, echo=FALSE}
fwd_mse_cv <- mean.cv.errors.fwd[2]
fwd_mse_cv
```


**Backward Stepwise**

```{r, echo=FALSE}
bwd <- regsubsets(crim~., data=Boston, subset=train, nvmax=13, method="backward")
```

```{r, echo=FALSE}
# Pick the best model using validation set approach
test_matrix <- model.matrix(crim~., data=Boston[test,])

val.errors <- rep(NA,13)
for(i in 1:13){
 coefi <- coef(bwd,id=i)
 pred <- test_matrix[,names(coefi)]%*%coefi
 val.errors[i] <- mean((Boston$crim[test]-pred)^2) 
}

which.min(val.errors)
```


```{r, echo=FALSE}
plot(val.errors, type='b')
points(which.min(val.errors), val.errors[8],col="red",cex=2,pch=20)
```

```{r, echo=FALSE}
bwd_mse <- val.errors[8]
bwd_mse
```

```{r, echo=FALSE}
# Perform backward subset selection with the chosen model on the entire dataset in order to obtain more accurate coefficient estimates

bwd_full <- regsubsets(crim ~., data=Boston, nvmax = 13)
coef(bwd_full, 8)
```

```{r, echo=FALSE}
best_subset_mse
```


```{r, echo=FALSE}
fwd_mse
```



```{r, echo=FALSE}
bwd_mse
```

* The backward selection model has the lowest test MSE when the model is selected using the validation set approach. It uses 2 more variables than best subset and forward selection - ptratio and lstat.


```{r, echo=FALSE}

predict.regsubsets <- function (object, newdata, id,...){
 form<-as.formula(object$call [[2]])
 mat<-model.matrix(form,newdata)
 coefi<-coef(object, id=id)
 xvars<-names(coefi)
 mat[,xvars]%*%coefi 
}

k <- 10
set.seed(4)
folds <- sample(1:k,nrow(Boston), replace=TRUE)
cv.errors <- matrix(NA,k,13, dimnames=list(NULL, paste(1:13)))

for(j in 1:k){
  best.fit <- regsubsets(crim~., data=Boston[folds!=j, ], nvmax=13, method='backward')
  for(i in 1:13) {
    pred<- predict(best.fit,Boston[folds==j,],id=i)
    cv.errors[j,i] <- mean((Boston$crim[folds==j]-pred)^2)
  }
}
```

```{r, echo=FALSE}
mean.cv.errors.bwd <- apply(cv.errors,2,mean) 
mean.cv.errors.bwd
```


```{r, echo=FALSE}
which.min(mean.cv.errors.bwd)
```


```{r, echo=FALSE}
plot(mean.cv.errors.bwd, type="b")
points(which.min(mean.cv.errors.bwd), mean.cv.errors.bwd[9], pch=20, col='red', cex=2)
```


```{r, echo=FALSE}
bwd_cv <- regsubsets(crim~., data=Boston, nvmax=13, method='backward')
coef(bwd_cv, 9)
```

```{r, echo=FALSE}
bwd_mse_cv <- mean.cv.errors.bwd[9]
bwd_mse_cv
```

* The backward stepwise selection model selected by cross validation has even lower test MSE.

**Lasso regression**

```{r, echo=FALSE}

set.seed(9)

x <- model.matrix(crim~., Boston)[, -1]
y <- Boston$crim

lasso.fit <- glmnet(x[train, ], y[train], alpha=1)
cv.lasso.fit <- cv.glmnet(x[train, ], y[train], alpha=1)
plot(cv.lasso.fit)
```

```{r, echo=FALSE}
bestlam.lasso <- cv.lasso.fit$lambda.min
bestlam.lasso
```


```{r, echo=FALSE}
lasso.pred <- predict(lasso.fit, s=bestlam.lasso, newx=x[test,]) 
lasso.error <- mean((lasso.pred-y[test])^2)
lasso.error
lasso.coef <- predict(lasso.fit,type="coefficients", s=bestlam.lasso)[1:13,]
length(lasso.coef[lasso.coef != 0])
```


```{r, echo=FALSE}
lasso.coef[lasso.coef != 0]
```


* Lasso regression zeroed out all the variables except for the intercept.

**Ridge regression**


```{r, echo=FALSE}
ridge.fit <- glmnet(x[train, ], y[train], alpha=0)
cv.ridge.fit <- cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv.ridge.fit)
```

```{r, echo=FALSE}
bestlam.ridge <- cv.ridge.fit$lambda.min
bestlam.ridge
```

```{r, echo=FALSE}
ridge.pred <- predict(ridge.fit, s=bestlam.ridge, newx=x[test,]) 
ridge.error <- mean((ridge.pred-y[test])^2)
ridge.error
```

* The ridge error is only slightly higher than the lasso

**PCR (Principal Component Regression)**


```{r, echo=FALSE}

set.seed(4)

pcr.fit <- pcr(crim~., data=Boston, subset=train, scale=TRUE, validation ="CV")
```

```{r, echo=FALSE}
summary(pcr.fit)
```

```{r, echo=FALSE}
validationplot(pcr.fit, val.type="MSEP")
```
```{r, echo=FALSE}
6.690^2 - 6.663^2
```


* The lowest MSE occurs when all 13 variables are present in the model, which doesn't help in dimensionality reduction. However, we can also see that between 8-13 variables, the test MSE for the 8 variable model is only higher by 0.360531.


```{r, echo=FALSE}
pcr.pred <- predict(pcr.fit, x[test,], ncomp=13) 
pcr.error <- mean((pcr.pred-y.test)^2)
pcr.error
```


**PLS (Partial Least Squares)

```{r, echo=FALSE}
set.seed(6)
pls.fit<- plsr(crim~., data=Boston, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
```

```{r, echo=FALSE}
validationplot(pls.fit, val.type="MSEP")
```


* The lowest MSE occurs when 9 variables are present.

```{r, echo=FALSE}
pls.pred <- predict(pls.fit, x[test,], ncomp=9) 
pls.error <- mean((pls.pred-y.test)^2)
pls.error 
```

**Findings:**

* Lasso regression has lower RMSE value than Ridge regression although its comparatively small

### (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error.


```{r, echo=FALSE}
errors <- c(lm.error, best_subset_mse, best_subset_mse_cv, fwd_mse, fwd_mse_cv, bwd_mse, bwd_mse_cv, lasso.error, ridge.error, pcr.error, pls.error)
names(errors) <- c("lm", "best subset val", "best subset cv","forward val", "forward cv", "backward val", "backward cv", "ridge", "lasso", "pcr", "pls")

par(las=2)
par(mar=c(5,8,4,2))
barplot(sort(errors, decreasing = T), horiz=TRUE)
```

```{r, echo=FALSE}
print(sort(errors))
```

**Findings**

* The best model is a 9 variable model selected using cross-validation and backward stepwise selection method.

* Ridge has all the variables considered with age and tax having a minimum effect based on their coefficient values whereas in Lasso regression, age and tax variables are eliminated due to their low effect

### (c) Does your chosen model involve all of the features in the data set? Why or why not?

* No, it does not. It contains 9 variables: zn, indus, nox, dis, rad, ptratio, black, lstat, and medv.




\newpage

# Chapter 8

## Question 8

### a) Split the data set into a training set and a test set.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
# ?Carseats
str(Carseats)

head(Carseats)
tail(Carseats)
train_split <-  sample(1:nrow(Carseats), round(nrow(Carseats) *0.75))
Carseats_train <-  Carseats[train_split, ]
Carseats_test <-  Carseats[-train_split, ]
```

### b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain?

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
tree_carseats <-  tree(Sales~ . , data=Carseats_train)
summary(tree_carseats)

tree_carseats$frame

plot(tree_carseats)
text(tree_carseats ,pretty =0,cex=0.5)

pred_carseats <-  predict(tree_carseats, Carseats_test)
mean((Carseats_test$Sales - pred_carseats)^2) 
```
* The test MSE is about 4.48


### c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
cv_carseats <-  cv.tree(tree_carseats, FUN=prune.tree)
plot(cv_carseats$size, cv_carseats$dev, type="b")

prune_car = prune.tree(tree_carseats, best = 5)
plot(prune_car)
text(prune_car,pretty=0,cex=0.4)


prune_predict=predict(prune_car, newdata= Carseats_test)
mean((prune_predict-Carseats_test$Sales)^2) 
```
*  The test MSE is 4.20 which is less than the one with prunning

### d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to de- termine which variables are most important.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
#Install all the required R packages

#Fits the data into the model 

My_bagged_model <- bagging(
  formula = Sales ~ .,
  data = Carseats_train,
  nbagg = 100,   
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0) 
)
My_bagged_model # The MSE is 2.599, which is a significant improvemnet from the other models
bag_predict <- predict(My_bagged_model,Carseats_test)


bag_carseats <-  randomForest(Sales~ . , data=Carseats_train, 
                              mtry=10, ntree=500, importance=TRUE)
bag_pred <-  predict(bag_carseats, Carseats_test)
mean((Carseats_test$Sales - bag_pred)^2)

importance(bag_carseats)


varImpPlot(bag_carseats)

rf_car = randomForest(Sales~.,data=Carseats_train,mtry = 10, importance = TRUE)
pred_rf = predict(rf_car,newdata=Carseats_test)
mean((Carseats_test$Sales - pred_rf)^2)
```

* As seen above, it looks like the price of the carseat and where it is located on
the shelf are the most important predictors of how a carseat will sell.

* Age, Competitor price, and advertising budget also appear to have an effect, but all
other variables seem to be less important

### e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which vari- ables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
mse.vec <- NA
for (a in 1:10){
  rf_carseats <-  randomForest(Sales ~ . , data=Carseats_train, 
                             mtry=a, ntree=500, importance=TRUE)
  rf_pred <-  predict(rf_carseats, Carseats_test)
  mse.vec[a] <- mean((Carseats_test$Sales - rf_pred)^2)
}

# best model
which.min(mse.vec)

mse.vec[which.min(mse.vec)]

```

* We see that the best model uses 10 variables at each split. That model decreases test error compared to bagging.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
rf_carseats <-  randomForest(Sales ~ . , data = Carseats, 
                             mtry=10, ntree=500, importance=TRUE)
importance(rf_carseats)
```

* ShelveLoc is the most important variable. Price, CompPrice, Advertising, and Age are also important predictors of Sale.

**Findings**:  

* ShelveLoc, Price are the two most important variables 

* error value forms a quadratic function while altering m value with a minimum at a certain of m  

<!-- ### f) Now analyze the data using BART, and report your results. -->
<!-- ```{r, echo=FALSE , warning = FALSE, message = FALSE,error=FALSE,fig.align='center'} -->
<!-- bartFit <- bart(Carseats_train,Carseats_train$Sales,Carseats_test) -->
<!-- bartFit -->
<!-- plot(bartFit) -->
<!-- ``` -->

## Question 11


### a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
 # summary(Caravan)
colnames(Caravan)
train_rows <- 1:1000
Caravan$Purchase <-  ifelse(Caravan$Purchase == "Yes", 1, 0)
train_Caravan <- Caravan[train_rows, ]
test_Caravan <- Caravan[-train_rows, ]
```

### b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
boost.caravan <-  gbm(Purchase ~ . , 
                      data=train_Caravan, 
                      n.trees=1000, shrinkage=0.01,
                      distribution="bernoulli")
boost.caravan
summary(boost.caravan)
```

* PPERSAUT, MKOOPKLA and MOPLHOOG are three most important variables in that order.

### c) Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated prob- ability of purchase is greater than 20 %. Form a confusion ma- trix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
prob <- predict(boost.caravan, newdata = test_Caravan, type = "response")
head(prob)

boost.pred <-  ifelse(prob >0.2, 1, 0)
table(test_Caravan$Purchase, boost.pred)

(32) /(103+32) # 23% of people predicted to make purchase actually end up making one.

fitcaravan <-  glm(Purchase ~ . , data=train_Caravan, family=binomial)
prob <-  predict(fitcaravan, test_Caravan, type="response")
                 

pred <-  ifelse(prob > 0.2, 1, 0)
table(test_Caravan$Purchase, pred)


(58)/(350+58) 
```

* 14% of people predicted to make purchase actually end up making one, which is less than the boosting one

* Actual positives out of predicted positives is good for knn but the total predictive is poor

* Boosting has a better prediction power with 21% positive prediction percentage

\newpage


# Chapter 10

## Question 7

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
summary(Default)

set.seed(1)
n <- nrow(Default)
ntest <- trunc(n/5)
testid <- sample(1:n, ntest)
y_test <- Default$default[testid] == 'Yes'
logistic_reg <- glm(default~student+balance+income,family="binomial",data=Default[-testid,])
logistic_pred <- predict(logistic_reg, data=Default[testid,], type='response') > 0.5
logistic_accuracy = mean(logistic_pred == y_test)
logistic_accuracy

```

*Fitting NN*

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
x = model.matrix(default ~. -1, data=Default)

x_train <- x[-testid,]
g_train <- Default$default[-testid]=='Yes'

x_test <- x[testid,]
g_test <- Default$default[testid] == 'Yes'

NN <- keras_model_sequential() %>% 
  layer_dense(units=10, activation='relu', input_shape=ncol(x)) %>%
  layer_dropout(rate=0.4) %>% 
  layer_dense(units = 1, activation='sigmoid')

NN %>% compile(
  optimizer=optimizer_rmsprop(), 
  loss='binary_crossentropy', 
  metrics='accuracy')

history <- NN %>% fit(
  x = x_train, 
  y = g_train, 
  epochs=30, 
  batch_size=128)

nnpred <- predict(NN, x_test) > 0.5
nn_accuracy <- mean(nnpred == g_test)
nn_accuracy
gc()
```
* As we can see, our neural network test set accuracy is about 96%. This is comparable to our linear logistic regression which had 95% accuracy.

\newpage

# Problem 1: Beauty Pays!

```{r, echo=FALSE , warning = FALSE, message = FALSE, fig.align='center'}
raw_data <- read.csv("BeautyData.csv")
## Checking the summary of the data
summary(raw_data)
## Plotting courseevals and beautyscore
train <- raw_data[,1:2]
k <- lm(BeautyScore ~ CourseEvals,train)
summary(k)
ggplot(train, aes(x=BeautyScore, y=CourseEvals)) + geom_point() + geom_smooth()
## Identifying the correlation between courseevaluations and beauty scores
cor(raw_data$CourseEvals,raw_data$BeautyScore)
ggplot(raw_data, aes(x=female, y=CourseEvals, group = female)) + geom_boxplot()   
ggplot(raw_data, aes(x=tenuretrack, y=CourseEvals, group = tenuretrack)) + geom_boxplot() 
ggplot(raw_data, aes(x=nonenglish, y=CourseEvals, group = nonenglish)) + geom_boxplot() 
```
* Gender has an effect on the course evaluation scores 


**b)** 

* It is impossible to accurately identify the effect of beauty on income.Skills and strengths of different people vary at different levels.It is quite difficult to keep the other factors constant while measuring the effect of beauty on the income.As teh measurement is based on humans, there are multipe subjective choices that are made based on the situation and type of job.Sometimes it is discrinimatory to say that a person got a job just because he/she is beautiful when he/she is equally talented than any other applicant for the job.Hence it is quite challenging to distinguish between productivity and discrimation in this regard.  

\newpage

# Problem: 2:Housing Price Structure

```{r,echo=FALSE}
set.seed(4)
midcity <- read.csv('MidCity.csv')
# Treating the data
#1.Drop the home variable as it is an index
midcity1 = midcity[,-1]
#2. Change the nbhd variable as a factor variable
midcity1$Nbhd = as.factor(midcity1$Nbhd)
housing.fit = lm(midcity1$Price ~.,midcity1)
#3. Create an interaction term between brick house and nbhd
housing.fit.inter = lm(midcity1$Price ~ Nbhd+Offers+SqFt+Brick+Bedrooms+Bathrooms+Brick*Nbhd,midcity1)
summary(housing.fit.inter)
```
1. Is there a premium for brick houses everything else being equal?  
* Yes there is a premium of $12093.05 if it is a brick house  
2. Is there a premium for houses in neighborhood 3?  
* Yes there is a premium of $16980 if it is nbhd 3
3. Is there an extra premium for brick houses in neighborhood 3?
* Yes, there is an extra premium of $11933
4. For the purposes of prediction could you combine the neighborhoods 1 and 2 into a
single older neighborhood?
* Yes we can combine neighborhood 1 adn 2 as they there is not much difference in the premium and the data shows that the nieghborhood 2 is not even statistically significant

\newpage

# Problem 3 : What causes what??
**1) **
*Washington D.C has a terror alert system which can be used to establish the causality between cops and crime but the other cities might or might not have it.By running a regression on crime and cops in other cities, we might end up proving the correlation between them but not the causation*

**2)**
*Researchers at Upenn used the terror alert system in Washington D.C to establish the causation. During the terror alert days, there are generally more cops deployed onto the streets. By comparing the correlation results between crime and cops, the researchers are able to conclude a causal relation between cops and crime.*

*From table 2,given the metro ridership is kept constant, the effect of high alert on crime is fairly constant.But we can clearly see that more the number of people on roads ,higher the crime rate.Also, introduction of metro ridership did not add much information as on high alert, it does not have a considerable effect*

**3)**
*Also, there was a hypothesis that the terror alert might be causing people not to come outside and the crimes might have dropped because there were less victims to fall prey. But the researchers used the metro ridership data to check if there is any decline in ridership on a terror alert which turned out that the ridership was fairly same when compared to normal days.*

**4)**
*In this model, the author was trying to show the interaction between high alert and the districts. There was a considerable between high alert and district 1 because additional police will be deployed on high alert days.But that might not be the case in other districts resulting in poor interaction between high alert and other districts*

\newpage

## Problem 6 : Describe your contribution to the project

#### Leadership
* I was able to guide the team towards the right approach to solve the problem. I made sure that the work that was being done was organized and structured so that there was no redundancy while performing multiple tasks.

* Apart from that, i played a key role in distributing the work among the team members to maintain the speed and made sure that we learnt from each others strengths.I encouraged the team members to ask for help when there is a roadblock

#### Analysis and Modelling
* With respect to model building, i have designed the EDA steps for the project  and ran iterations on models like logistic regression, random forests, KNN classification to apply the learnings from the class in the project.

* Collaboration on sharing the insights and the code on github was my idea to successfully complete the project without any obstacles.

#### Presentation
* With respect to the story flow of the presentation, i suggested some key points like talking headers, simplicity/brevity of the slides to convey the right information in the right amount of time.

\newpage

# References
1). Rpubs - https://rpubs.com/

2). Nueral Networks - https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/

3). ISLR Book PDF Version - https://hastie.su.domains/ISLR2/ISLRv2_website.pdf

4). Professor Carlos Class notes - https://sites.google.com/view/predictive-modeling/home

